\documentclass[11pt, letterpaper]{article}
\usepackage[margin=2cm]{geometry}
\usepackage{fourier}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage{bookmark}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{parskip}
\usepackage{epstopdf}
\usepackage[sorting=none,style=ieee]{biblatex}
\DeclareUnicodeCharacter{2009}{\,}
\setcounter{secnumdepth}{-\maxdimen}

\title{cuTimeWarp: Accelerating Soft Dynamic Time Warping on GPU}

\author{Alex Kyllo \and Afrooz Rahmati}

\addbibresource{cuTimeWarp.bib}

\begin{document}

\maketitle

\begin{abstract}

This report explores techniques for optimizing the computation of Soft Dynamic
Time Warping, a differentiable sequence dissimilarity measure, on graphics
processing units (GPU), for the purpose of enabling high-performance machine
learning on time series datasets.

\end{abstract}

\section{Introduction}

Time series machine learning is a research area with countless useful
applications such as recognizing sounds and gestures. Clustering or classifying
large time series datasets is challenging partly because of the need to define a
measure of dissimilarity between any two given time series. Furthermore,
practical applications require finding common structure in time series despite
different speeds or phases; for example, a word means the same whether spoken
quickly or slowly. Another requirement for machine learning tasks is that the
dissimilarity measure must be differentiable so that its gradient can be used as
to minimize it as a loss function to find the best fit model. Finally, the
measure must be efficient to calculate, because it will be calculated repeatedly
many times during model fitting. To this end we will explore GPU acceleration of
Soft Dynamic Time Warping (Soft-DTW) \cite{cuturi_soft-dtw_2018}, a
differentiable sequence dissimilarity measure, to enable high performance time
series machine learning.

\subsection{Background}

TODO: add an intuitive explanation of what time series data is

There are multiple valid ways to compute a measure of dissimliarity between two
time series; the simplest is Euclidean distance, which is the square root of the
sum of squared pairwise differences between two time series $x$ and $y$, each of
length $n$ (equation \ref{euclid}).

\begin{equation} \label{euclid}
d(x, y) = \sqrt{\sum_{t=1}^{n}(x_t-y_t)^2}
\end{equation}

The drawback of Euclidean distance in time series applications is that two
structurally similar time series will produce a large distance if they are
different speeds or out of phase (TODO: Add a figure to illustrate this).
Dynamic Time Warping (DTW) was devised in the 1960s as an alternative time
series dissimilarity measure to address this shortcoming. (TODO: find a
historical citation)

The basic algorithm for DTW is to use Bellmanâ€™s recursion, a dynamic programming
technique, to find the lowest-cost path diagonally across a pairwise distance
matrix. The computation cost for this approach is quadratic ($O(mn)$) for time
series vectors of length m and n \cite{cuturi_soft-dtw_2018}. The formula for
the DTW between time series x and y is given by equation \ref{dtw}.
(TODO: explain the algorithm in more detail and provide a visualization)

\begin{equation} \label{dtw}
DTW(x,y) = min_{\pi}\sqrt{\sum_{(i,j)\in\pi}d(x_{i},y_{j})^2}
\end{equation}

Where $d(x_i,y_j)^2$ is the cost function, typically pairwise squared Euclidean
distance. The loss function for DTW is not differentiable due to the min
operation within the formula; a small change in the input time series may result
in zero change in the path cost. However, we can create a differentiable version
called Soft-DTW by replacing the min function with a soft-min function
(equation \ref{softdtw}) \cite{cuturi_soft-dtw_2018}.

\begin{equation} \label{softdtw}
\text{soft-min}_\gamma(a_1,...,a_n) = -\gamma log\sum_{i}e^{-a_i/\gamma}
\end{equation}

Hence, Soft-DTW is parameterized by the smoothing constant gamma, which becomes
a tunable hyperparameter in machine learning model training applications.

A common technique in machine learning with Soft-DTW is the computation of
barycenters, which are centroids within the space of a set of time series. The
differentiability of Soft-DTW allows for barycenter finding via gradient
descent, and then new observations can be classified by finding the nearest
barycenter. Sequence prediction and generation of is also possible using
recurrent neural networks with Soft-DTW as a loss function
\cite{cuturi_soft-dtw_2018}.

(TODO: Explain process of barycenter finding with gradient descent on softdtw
loss)

Prior to computing the Soft-DTW dissimilarity between any two time series, each
time series should be \emph{z-normalized}, that is, scaled so that its mean is
equal to 0 and its standard deviation is equal to 1, to remove the problem of
``wandering baselines'' or ``drift'' in the measurements, as illustrated in
\cite{rakthanmanon_addressing_2013} with an ECG classifier that yields incorrect
results on un-normalized data due to drift that ``may be caused by patient
movement, dirty lead wires/electrodes, loose electrodes, and so on,'' and which
``does not have any medical significance.'' Z-normalization also tends to make
the iterative process of minimizing the cost function through gradient descent
or Newtonian methods more efficient because its hyperplane is not
disproportionately stretched in any one dimension, so the step size in any
direction is the same relative to the scale of that dimension. (TODO: explain
this better, find a citation)

\subsection{Related Work}

One important optimization we can apply is the use of Sakoe-Chiba bands, which
prune the warping path search space, allowing path finding in subquadratic time.
While this technique produces an approximation of the optimal path, in practice
it has been shown to improve task performance by preventing pathological
alignments where a very small portion of one time series maps onto a large
portion of another \cite{keogh_exact_2002}.

Utilizing indexing to construct lower bounds on warping distance is an
optimization technique for speeding up nearest neighbor search via early removal
of poor candidates \cite{keogh_exact_2002}. Shen and Chi (2021) proposes an
optimization of nearest neighbor search of multivariate time series, leveraging
the triangle inequality and quantization-based point clustering to restrict the
search \cite{shen_tc-dtw_2021}.

Xiao et al (2013) introduced a prefix computation technique for transforming the
diagonal data dependence to improve parallel computation of the cost matrix on
GPU \cite{xiao_parallelizing_2013}. Zhu et al (2018) demonstrates a method of
optimizing  memory access by taking advantage of the diagonal data dependency to
rearrange the matrix so that elements on the same diagonal are stored
contiguously \cite{zhu_developing_2018}. A prior implementation of Soft-DTW on
CUDA using PyTorch and Numba is capable of 100x performance improvement over the
original Soft-DTW Cython code, but is limited to sequence lengths of 1024 (CUDA
max block size) and leaves many opportunities for further CUDA optimizations
such as the use of shared memory \cite{maghoumi_pytorch-softdtw-cuda_2021}. A
2015 paper describes a tiling approach called \emph{PeerWave}, which utilizes
direct synchronization between neighboring streaming multiprocessors (SMs) to
handle the inter-tile data dependency without atomic operations, locks, or other
global barriers, leading to improved load balance and scaling properties
\cite{belviranli_peerwave_2015}.

In our project we will focus on this area of opportunity, optimizing matrix
structure and memory access patterns to maximize parallelism and minimize memory
latency in the computation of the warping path matrix.

\section{Methods}

To evaluate various performance optimizations on the Soft-DTW computation, we
implemented a C++ and CUDA library called cuTimeWarp, which includes functions
for computing the SoftDTW on CPU and GPU.

Given a set of many multivariate time series of the same length and number of
variables, we can compute the Soft-DTW distance between every time series and
every other time series in the set by computing, in parallel for each pair, a
pairwise squared Euclidian matrix, then applying the Soft-DTW
calculation on the distance matrix.

\section{Results}

\section{Discussion}

\section{Future Work}

Our library provides CUDA kernels and C++ wrappers for computing pairwise
Soft-DTW dissimilarity measures as well their gradients in parallel. Potential
future work includes integrating this library with an optimization library that
can iteratively minimize Soft-DTW cost to find barycenters among a set of time
series, to assemble a nearest centroid classification system. Another area of
potential is writing Python bindings to expose the Soft-DTW loss and gradient
functions to deep learning frameworks such as TensorFlow or PyTorch, to enable
the use of Soft-DTW loss as a training objective for recurrent neural networks.

\printbibliography[]
\end{document}
