\documentclass[11pt, letterpaper]{article}
\usepackage[margin=2cm]{geometry}
\usepackage{fourier}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage{bookmark}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{parskip}
\usepackage{epstopdf}
\usepackage[sorting=none,style=ieee]{biblatex}
\DeclareUnicodeCharacter{2009}{\,}
\setcounter{secnumdepth}{-\maxdimen}

\title{cuTimeWarp: Accelerating Soft Dynamic Time Warping on GPU}

\author{Alex Kyllo \and Afrooz Rahmati}

\addbibresource{cuTimeWarp.bib}

\begin{document}

\maketitle

\begin{abstract}

This report explores techniques for optimizing the computation of Soft Dynamic
Time Warping, a differentiable sequence dissimilarity measure, on graphics
processing units (GPU), for the purpose of enabling high-performance machine
learning on time series datasets.

\end{abstract}

\section{Introduction}

Time series machine learning is a research area with countless useful
applications such as recognizing sounds and gestures. Clustering or classifying
large time series datasets is challenging partly because of the need to define a
measure of dissimilarity between two time series. Practical applications require
finding common structure despite different speeds or phases; a word means the
same whether spoken quickly or slowly. Another requirement is that the measure
must be differentiable so that its gradient can be used as a loss function to
minimize in model fitting. Finally, the measure must be efficient to calculate.
To this end we will explore GPU acceleration of Soft Dynamic Time Warping
(Soft-DTW) \cite{cuturi_soft-dtw_2018}, a differentiable dissimilarity measure,
to enable high performance time series machine learning.

Dynamic Time Warping (DTW) is an algorithm to compute the dissimilarity between
two time series, which may vary in speed and phase. The basic algorithm for DTW
is to use Bellmanâ€™s recursion, a dynamic programming technique, to find the
lowest-cost path diagonally across a pairwise distance matrix. The computation
cost for this approach is quadratic(mn) for time series vectors of length m and
n \cite{cuturi_soft-dtw_2018}. The formula for the DTW between time series x and
y is: 

$$DTW(x,y) = min_{\pi}\sqrt{\sum_{(i,j)\in\pi}d(x_{i},y_{j})^2}$$

Where $d(x_i,y_j)^2$ is the cost function, typically pairwise squared Euclidean
distance. The loss function for DTW is not differentiable due to the min
operation within the formula; a small change in the input time series may result
in zero change in the path cost. However, we can create a differentiable version
called Soft-DTW by replacing the min with a soft-min function
\cite{cuturi_soft-dtw_2018}:

$$\text{soft-min}_\gamma(a_1,...,a_n) = -\gamma log\sum_{i}e^{-a_i/\gamma}$$

Hence, Soft-DTW is parameterized by the smoothing constant gamma, which becomes
a tunable hyperparameter in machine learning model training applications.

A common technique in machine learning with Soft-DTW is the computation of
barycenters, which are centroids within the space of a set of time series. The
differentiability of Soft-DTW allows for barycenter finding via gradient
descent, and then new observations can be clustered or classified by finding the
closest barycenter.

\section{Related Work}

One important optimization we can apply is the use of Sakoe-Chiba bands, which
prune the warping path search space, allowing path finding in subquadratic time.
While this technique produces an approximation of the optimal path, in practice
it has been shown to improve task performance by preventing pathological
alignments where a very small portion of one time series maps onto a large
portion of another \cite{keogh_exact_2002}.

Utilizing indexing to construct lower bounds on warping distance is an
optimization technique for speeding up nearest neighbor search via early removal
of poor candidates \cite{keogh_exact_2002}. Shen and Chi (2021) proposes an
optimization of nearest neighbor search of multivariate time series, leveraging
the triangle inequality and quantization-based point clustering to restrict the
search \cite{shen_tc-dtw_2021}.

Xiao et al (2013) introduced a prefix computation technique for transforming the
diagonal data dependence to improve parallel computation of the cost matrix on
GPU \cite{xiao_parallelizing_2013}. Zhu et al (2018) demonstrates a method of
optimizing  memory access by taking advantage of the diagonal data dependency to
rearrange the matrix so that elements on the same diagonal are stored
contiguously \cite{zhu_developing_2018}. A prior implementation of Soft-DTW on
CUDA using PyTorch and Numba is capable of 100x performance improvement over the
original Soft-DTW Cython code, but is limited to sequence lengths of 1024 (CUDA
max block size) and leaves many opportunities for further CUDA optimizations
such as the use of shared memory \cite{maghoumi_pytorch-softdtw-cuda_2021}. A
2015 paper describes a tiling approach called \emph{PeerWave}, which utilizes
direct synchronization between neighboring streaming multiprocessors (SMs) to
handle the inter-tile data dependency without atomic operations, locks, or other
global barriers, leading to improved load balance and scaling properties
\cite{belviranli_peerwave_2015}.

In
our project we will focus on this area of opportunity, optimizing matrix
structure and memory access patterns to maximize parallelism and minimize memory
latency in the computation of the warping path matrix.

\section{Methods}

\section{Results}

\section{Discussion}

\printbibliography[]
\end{document}