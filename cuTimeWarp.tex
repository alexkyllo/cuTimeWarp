\documentclass[12pt, letterpaper]{article}
\usepackage[margin=2cm]{geometry}
\usepackage{fourier}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage{bookmark}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{parskip}
\usepackage{epstopdf}
\usepackage{pgf}
\usepackage{pgfplots}
\usepackage[sorting=none,style=ieee]{biblatex}
\usepackage[utf8]{inputenc}\DeclareUnicodeCharacter{2212}{-}
\DeclareUnicodeCharacter{2009}{\,}
\setcounter{secnumdepth}{-\maxdimen}

\title{cuTimeWarp: Accelerating Soft Dynamic Time Warping on GPU}

\author{Alex Kyllo \and Afrooz Rahmati}

\addbibresource{cuTimeWarp.bib}

\begin{document}

\maketitle

\begin{abstract}

In this report we explore techniques for optimizing the parallel computation of
Soft Dynamic Time Warping, a differentiable sequence dissimilarity measure, on
graphics processing units (GPU), for the purpose of enabling high-performance
machine learning on time series datasets.

\end{abstract}

\section{Introduction}

Time series machine learning is a research area with countless useful
applications such as recognizing sounds and gestures. Clustering or classifying
large time series datasets is challenging partly because of the need to define a
measure of dissimilarity between any two given time series, which is not as
straightforward as computing distance between independent points in dimensional
space. Furthermore, practical applications require finding common structure in
time series despite different speeds or phases; for example, a word means the
same whether spoken quickly or slowly. Another requirement for machine learning
tasks is that the dissimilarity measure must be differentiable so that its
gradient can be used as to minimize it as a loss function to find the best fit
model. Finally, the measure must be efficient to calculate, because it will be
calculated repeatedly many times during the model fitting process. To this end
we will explore the GPU acceleration of Soft Dynamic Time Warping (Soft-DTW)
\cite{cuturi_soft-dtw_2018}, a differentiable sequence dissimilarity measure, to
enable high performance time series machine learning.

\subsection{Background}

\subsubsection{Time series data}

Time series data generally refers to data containing quantitative measurements
collected from the same subject or process at multiple points in time, and it
exhibits several peculiarities in comparison to time-independent data.  Time
series data can exhibit autocorrelation, meaning that the value at any point in
time can exhibit correlation to the value at a previous point in time, with some
delay or \emph{lag} in between. Time series data can also exhibit
\emph{seasonality} or cyclical patterns as well as \emph{trend} which is a
tendency for the average value (over some rolling time window) to increase or
decrease as a function of time. Due to these characteristics, time series data
does not conform to the I.I.D. (independent and identically distributed)
assumption that typically applies in the study of random variables, and
therefore it requires special techniques for analysis and modeling.

Time series data can be either univariate or multivariate. For example, a set of
electrocardiogram (ECG) measurements has a single variable (heart voltage)
collected over time, but if the dataset also included the patient's blood
pressure and oxygen levels measured at each time point, that would constitute a
multivariate time series, and correlations between the different variables could
be studied in addition to the autocorrelation within each of the variables.

\subsubsection{Time series distance and dissimilarity measures}

A fundamental capability that enables learning from data is the ability to
quantify a metric of distance or dissimilarity between observations, because
this allows comparison among data observations as well as quantification of
error between observed data and the predictions of an estimator model. For
time-independent data, multiple valid notions of distance exist; the most
commonly used is Euclidean distance, but others such as Manhattan distance,
cosine distance, or Mahalanobis distance are often used depending on the problem
domain.

Likewise, there are multiple valid ways to compute a measure of dissimliarity
between two sequences or time series; for real-valued time series measurements
the simplest is also Euclidean distance, which is the square root of the sum of
squared pairwise differences between two time series $x$ and $y$, each of length
$n$ (equation \ref{euclid}).

\begin{equation} \label{euclid}
d(x, y) = \sqrt{\sum_{t=1}^{n}(x_t-y_t)^2}
\end{equation}

\begin{wrapfigure}[12]{R}{0.3\textwidth}
  \centering
  \includegraphics[height=0.25\textwidth]{img/peaks.png}
  \caption{Euclidean vs. DTW on out of phase time series \cite{rakthanmanon_addressing_2013}}
  \label{peaks}
\end{wrapfigure}

However, a significant drawback of Euclidean distance in time series
applications is that two structurally similar time series will produce a large
distance if they are at different speeds or out of phase. In time series
applications it is often desirable to produce a low dissimilarity for
structurally similar time series despite variations in phase or speed, so an
alternative method of quantifying dissimilarity is needed. This is where Dynamic
Time Warping (DTW) comes in, providing an optimal nonlinear pairwise matching
path (Figure \ref{peaks}).


\subsubsection{Dynamic Time Warping}

Dynamic Time Warping (DTW) was devised in the 1960s as an alternative time
series dissimilarity measure to address this shortcoming
\cite{sakoe_dynamic_1978}. DTW is a nonlinear mapping from each point in one
time series to the nearest point in a second time series; by allowing this
nonlinearity, the algorithm is able to minimize the computed distance
between time series that are similar in shape but misaligned in the time domain.
While DTW is technically not considered a ``distance'' because it does not
conform to the triangle inequality \cite{jain_semi-metrification_2018}, and
therefore we refer to it as a ``dissimilarity'' instead, it can still be used in
place of Euclidean distance or other distance measures for many practical
applications of time series data.

The problem can be visualized as a matrix of pairwise distances between every
element of time series $x$ and every element of time series $y$. Under this
representation, the Euclidean distance measure translates to the shortest
diagonal path across the matrix from one corner to the other, while DTW allows
the path to ``wander'' from the diagonal in search of the lowest cost path
(Figure \ref{euclidean_matrix}), subject to the constraints that the path must
begin at one corner and end at the opposite corner, and make monotonic progress
in between.

\begin{figure}[htbp]
\includegraphics[height=2in]{img/euclidean_dtw_matrix.png}
\centering
\caption{Euclidean Distance vs. DTW represented as a shortest
  vs. lowest-cost diagonal path across a pairwise distance matrix}
\label{euclidean_matrix}
\end{figure}

The basic algorithm for DTW is to use Bellman's recursion, a dynamic programming
technique, to find the lowest-cost path diagonally across a pairwise distance
matrix; at each step, the cost is updated to the current cell's cost plus the
minimum of the three previous adjacent cells' costs. The computation cost for
this approach is quadratic ($O(mn)$) for time series vectors of length m and n
\cite{cuturi_soft-dtw_2018}. The formula for the DTW between time series x and y
is given by equation \ref{dtw}.

\begin{equation} \label{dtw}
DTW(x,y) = min_{\pi}\sqrt{\sum_{(i,j)\in\pi}d(x_{i},y_{j})^2}
\end{equation}

Where $d(x_i,y_j)^2$ is the cost function, typically pairwise squared Euclidean
distance between an element of time series $x$ and an element of time series
$y$. The loss function for DTW is not differentiable due to the min operation
within the formula; a small change in the input time series may result in zero
change in the path cost. However, we can create a differentiable version called
Soft-DTW by replacing the min function with a soft-min function (equation
\ref{softdtw}) \cite{cuturi_soft-dtw_2018}.

\begin{equation} \label{softdtw}
\text{soft-min}_\gamma(a_1,...,a_n) = -\gamma log\sum_{i}e^{-a_i/\gamma}
\end{equation}

Hence, Soft-DTW is parameterized by the smoothing constant gamma, which becomes
a tunable hyperparameter in machine learning model training applications.

\subsubsection{Applications}

DTW is a widely used technique employed in many areas of science, including
biology, technology, economics, and finance. DTW can be used to discover
patterns in time series or search within signal databases to find the closest
matches \cite{keogh_derivative_2001}. It is also utilized in machine learning
applications like clustering, regression, and classification on time series
data. Practical applications of DTW include tasks such as:

\begin{itemize}
    \item Voice command recognition
    \item Sound detection and classification
    \item Handwriting and gesture recognition
    \item Human activity recognition
    \item Heart rhythm anomaly detection
    \item Sales or asset price pattern detection
\end{itemize}

As a differentiable time series dissimilarity measure, DTW can be applied as a
loss function or minimization objective in data modeling techniques such as:

\begin{itemize}
  \item \emph{k}-means Clustering
  \item Hierarchical agglomerative clustering
  \item Motif (similar subsequence) search
  \item k-nearest neighbor or nearest centroid classification
  \item Recurrent neural networks
\end{itemize}

A common technique in machine learning with Soft-DTW is the computation of
barycenters, which are centroids within the space of a set of time series. The
differentiability of Soft-DTW allows for barycenter finding via gradient descent
or semi-Newtonian function minimization methods. First, a candidate time series
is initialized with random points, and then, iteratively, its Soft-DTW
dissimilarity to a set of multiple time series representing a target class is
calculated, then the gradient of the Soft-DTW dissimilarity is taken with
respect to the input values, and the inputs are updated by a small step size in
the direction of the gradient, until convergence. After the best-fit barycenters
are found, new observations can be classified by finding the nearest barycenter.
Sequence prediction and generation is also possible using recurrent neural
networks with Soft-DTW as a loss function \cite{cuturi_soft-dtw_2018}.

Prior to computing the Soft-DTW dissimilarity between any two time series, each
time series should be \emph{z-normalized}, that is, scaled so that its mean is
equal to 0 and its standard deviation is equal to 1, to remove the problem of
``wandering baselines'' or ``drift'' in the measurements, as illustrated in
\cite{rakthanmanon_addressing_2013} with an ECG classifier that yields incorrect
results on un-normalized data due to drift that ``may be caused by patient
movement, dirty lead wires/electrodes, loose electrodes, and so on,'' and which
``does not have any medical significance.'' Z-normalization also tends to make
the iterative process of minimizing the cost function through gradient descent
more efficient because it prevents parameters with larger magnitude from
dominating the direction of the weight update step
\cite{jordan_normalizing_2018}. For this reason, all input data used in our
performance experiments is z-normalized in advance.

\subsubsection{Parallelizing DTW}

A naive, sequential implementation of DTW would involve a nested loop to iterate
over each row and column of the cost matrix to update each cell's cost based on
the three neighboring cells' costs, hence the $O(n^2)$ time complexity. Because
each cell has a data dependency on the three cells to the top, left, and
top-left, there is no dependency between cells that are on the same antidiagonal
of the matrix, therefore computation of these cells can be handled by parallel
threads. One thread computes the upper-leftmost cell, then two threads compute
the next antidiagonal, then three threads compute the next, and so on. Because
the length of the longest antidiagonal of a matrix is $min(m,n)$ that is also
the number of threads that can be utilized in computing parallel DTW, and a
subset of these threads will be inactive in computing antidiagonals that are
shorter than the longest antidiagonal.

\subsection{Related Work}

Because DTW has so many useful data mining applications but suffers from
quadratic complexity, scholars have devised a wide variety of exact and
approximate approaches to improve its performance.

Dr. Eamonn Keogh and several others have utilized various indexing schemes to
construct lower bounds on warping distance as an optimization technique for
speeding up nearest neighbor search via early removal of poor candidates
\cite{keogh_exact_2002}.

Shen and Chi (2021) propose an optimization of nearest neighbor search of
multivariate time series, leveraging the triangle inequality and
quantization-based point clustering to restrict the search
\cite{shen_tc-dtw_2021}.

Xiao et al (2013) introduced a prefix computation technique for transforming the
diagonal data dependence to improve parallel computation of the cost matrix on
GPU \cite{xiao_parallelizing_2013}.

Zhu et al (2018) demonstrate a method of optimizing memory access by taking
advantage of the diagonal data dependency to rearrange the matrix so that
elements on the same diagonal are stored contiguously
\cite{zhu_developing_2018}.

Salvador and Chan (2004) \cite{salvador_fastdtw_2004} proposed a method of
accelerating and approximation of DTW, dubbed FastDTW, by first downsampling the
time series to a fraction of its length to find an approximate best path and
then recursively upsampling and reapplying the algorithm, using the previous
lower-resolution best path to construct upper and lower bounds for next
pass. However, Wu and Keogh (2020) demonstrate that this method is generally
slower than exact DTW on most realistic datasets.

A prior implementation of Soft-DTW on CUDA using PyTorch and Numba claims to
achieve 100x performance improvement over the original Soft-DTW reference
implementation in Cython code, but is limited to sequence lengths of 1024 (CUDA
maximum thread block size) and leaves many opportunities for further CUDA
optimizations such as the use of shared memory
\cite{maghoumi_pytorch-softdtw-cuda_2021}.

A 2015 paper describes a tiling approach called \emph{PeerWave}, which utilizes
direct synchronization between neighboring streaming multiprocessors (SMs) to
handle the inter-tile data dependency without atomic operations, locks, or other
global barriers, leading to improved load balance and scaling properties
\cite{belviranli_peerwave_2015}. This tiling strategy also enables the PeerWave
algorithm to handle longer time series than 1024.

In our work, we primarily focus on this general area of opportunity, optimizing
DTW cost matrix structure and memory access patterns to increase parallelism and
minimize memory latency in the computation of the warping path matrix.

\section{Methods}

To evaluate various performance optimizations on the Soft-DTW computation, we
implemented a C++ and CUDA library called cuTimeWarp, which includes functions
for computing the SoftDTW on CPU and GPU.

Given a set of many multivariate time series of the same length and number of
variables, we can compute the Soft-DTW distance between every time series and
every other time series in the set by computing, in parallel for each pair, a
pairwise squared Euclidian matrix, then applying the Soft-DTW calculation on the
distance matrix. This computation, however, also has an $O(n^2)$ complexity and
can potentially even take longer than the DTW computation itself. For univariate
time series, we can save this cost by computing the DTW cost matrix on the two
input time series directly, computing the absolute distance between each pair of
values from the two time series within the nested loop of the DTW procedure.

\subsection{Optimization Techniques}

We opted to experiment with the following performance optimization techniques:

\begin{enumerate}
\item Wavefront tiling (based on PeerWave)
\item Diagonal-major data layout
\item Shared memory stencil
\item Sakoe-Chiba bands
\end{enumerate}

Before delving into the performance results, we will provide a brief explanation
of how each of these techniques is implemented and why it provides a performance
improvement.

\subsubsection{Wavefront Tiling}
Wavefront parallelism is one of the useful methods to overcome the dependencies
of nested loops by multiple processing units. The idea is to re-order the loop
iterations in such a way that form diagonal wave and each wave can be computed
in parallel. Barriers will be utilized to control the data dependencies among
consecutive waves. Elements inside the wave grouped together using tiled
technique to enhance data locality and performance. This methodology presents a
second degree of parallelism where tiles can be computed in parallel by
separating with a global variable.

GPU and specifically CUDA can accelerate the process of wavefront. Each tile
assigns to a block to be process in parallel by SM. On the other hand, the
iteration along diagonals within a tile are also pluralized. In order to enforce
the dependencies, the global barriers used among the tiles and within every
tiles \cite{belviranli_peerwave_2015}.

In our Soft DTW implementation, we utilized the wavefront technique to manage
the dependencies of neighboring cells for computing the minimum warp path. In
our algorithm this value depends on the minimum cost of the upper, left and
upper-left diagonal cell cost. Figure \ref{DTW_dependency} show this
dependencies clearly.Each D(i,j) depends on the up, left and up-left neighbor
cost.

\begin{figure}[htbp]
\includegraphics[height=2in]{img/tiling_dependencies.png}
\centering
\caption{computation dependencies for DTW \cite{belviranli_peerwave_2015}.}
\label{DTW_dependency}
\end{figure}

Wavefront process split to three major steps:

\begin{itemize}
  \item Populating the dependencies managing by loop. In this step, we keep the
    global variable WaveId to sperate the process of waves. Wavelength is
    increasing one by one on each loop iterations until reaching the total
    number of tiles. The primary kernel softdtw global tiled called with the
    wavelength number of blocks and thread size of tile width.
  \item In this kernel, the row and column index for each tile is calculating
    and passing as a global variable to the corresponding kernel for the tilesâ€™
    computation.
  \item The third step is the main kernel to process inter-tiles in
    parallel. Shared memory employed to keep the tile within the cache and
    improve the overall performance. As we mentioned earlier, we need to
    calculate the soft-min for the dependent cells. Therefore, the soft-min
    calculated for the up, left and diagonal upper left index.(equation
    \ref{dependencies})
\end{itemize}

\begin{equation} \label{dependencies}
  D (i,j) = D(i-1, j-1) + Soft-min
  \begin{cases}
        D(i-1,j) \\
        D(i,j-1)\\
        D(i-1,j-1)
  \end{cases}
\end{equation}
Drawbacks: Due to the usage of global barriers, there would be lower utilization
for the GPU.  Fewer threads would be available at the beginning and at the end
of each tiles process.  Also, less tiles are accessible toward the beginning and
end of wave iterations.  Therefore, device SM remain idle during the initiation
and end of the process.

\subsubsection{Diagonal-major layout}

Because the data dependency structure of the DTW algorithm results in elements
of the distance and cost matrices on the same antidiagonal being processed in
parallel, storing these matrices in row-major or column major order will cause a
performance impact from cache misses and non-coalesced accesses to global
memory.

If the data is first rearranged into an antidiagonal-major layout, then at each
iteration of the wavefront loop, processor threads will make coalesced accesses
to data elements that are laid out contiguously in memory. As illustrated in
Figure \ref{diagonal_layout}, this transforms an $m \times n$ matrix that must be
iterated over diagonally, into a $(m+n-1) \times (min(m,n))$ matrix that can be
iterated from top to bottom, with one thread assigned to each column. At each
iteration step (i.e. each row of the diagonal-major matrix), contiguous array
elements are read from memory into cache and written from cache back to memory,
resulting in coalesced accesses and fewer cache misses.

\begin{figure}[htbp]
  \includegraphics[height=2.5in]{img/diagonal_layout.png}
  \centering
  \caption{Cost matrix transformation to antidiagonal-major layout (arrows show
    iteration direction)}
  \label{diagonal_layout}
  \end{figure}

\subsubsection{Shared memory stencil computation}

As the program iterates diagonally across the distance matrix to find the
optimal warping path, each cell in the path utilizes the previously computed
results of three previous neighboring cells; if the iteration is visualized as
proceeding from the upper left to the lower right corner of the matrix, the cost
value in each cell depends on the (soft) minimum of the costs in the cell
above, the cell to the left, and the cell to the diagonal upper-left, which were
computed in the previous two iterations (Figure \ref{cost_deps}). If the cost
matrix \verb|R| resides in global memory, then non-contiguous accesses to
\verb|R[i-1][j]|, \verb|R[i][j-1]| and \verb|R[i-1][j-1]| will result in cache
misses, incurring a significant performance cost. Since each element of \verb|R|
will be referenced up to three times in the computation of dependent cells,
these cache misses can be avoided via a stencil computation using shared memory
in CUDA. The stencil serves as a cache for the current and previous two
diagonals; once a diagonal is no longer in use, its elements are written back to
the cost matrix \verb|R| in device global memory.

\FloatBarrier
The algorithm can be modified to use shared memory as follows:

\begin{verbatim}
D is a squared euclidean distance matrix of two time series
R is a cost matrix initialized to positive infinity except for R[0, 0] = 0
for each anti-diagonal of R starting from R[0, 0]
    if the current thread index < length of the current anti-diagonal
        copy R[i][j] from global memory into the stencil array
        read R[i-1][j], R[i][j-1] and R[i-1][j-1] from the stencil array
        compute cost as softmin(R[i-1][j], R[i][j-1], R[i-1][j-1]) + D[i-1][j-1]
        write the cost back to the stencil
        copy the cost in R[i-1][j-1] from the stencil back to global memory
\end{verbatim}
\FloatBarrier

\subsubsection{Sakoe-Chiba bands}

Sakoe-Chiba bands, proposed by Sakoe and Chiba in their 1978 paper "Dynamic
programming algorithm optimization for spoken word recognition," introduce a
"slope constraint" to the warping path to limit the maximum allowed warping
distance beyond which a pair will not be considered in the optimal path
calculation \cite{sakoe_dynamic_1978}. Pruning the search space by removing some
of the extreme diagonals from consideration yields an approximation of the
optimal warping path that can be calculated in sub-quadratic time.

This optimization is simple to implement for square matrices (i.e. DTW on time
series of equal length) by checking that the absolute difference between the
loop counter variables \verb|i| and \verb|j| does not exceed a fixed bandwidth
threshold value (Figure \ref{sakoe_chiba}). For rectangular matrices, since the
leading diagonal does not end at the lower right corner, the implementation must
be slightly modified to ensure that the counter variable along the longer of the
two dimensions stays within a defined radius. Either way the result is a
diagonal band matrix.

In a parallel programming environment such as CUDA, this optimization can also
allow for the computation of the warping path using fewer threads, as threads
assigned to cost matrix cells outside the band would go unused. Space savings
can also be obtained if the bandwidth is known in advance, by storing the
distance matrix and cost matrix in band storage format, omitting the zeroes
in the corners.

While this technique produces only an approximation of the optimal path, in
practice it has been shown to improve task performance by preventing
pathological alignments where a very small portion of one time series maps onto
a large portion of the other \cite{keogh_exact_2002}. The width of the band
can be a tunable hyperparameter for time series classification tasks.

\begin{figure}[htbp]
\includegraphics[height=2in]{img/sakoe_chiba.png}
\centering
\caption{Illustration of one possible optimal DTW path for a length 5 series and
a length 8 series with Sakoe-Chiba band radius = 1.}
\label{sakoe_chiba}
\end{figure}

\subsection{Complexity Analysis}

In order to quantify the performance of our implementations as a floating point
operation execution rate, we needed to count the floating-point operations used
to compute the cost matrix. Calculating a single cell of the cost matrix in
Soft-DTW involves a softmin function of three real-valued arguments. The softmin
function for three arguments is implemented in C++ as:

\begin{verbatim}
float softmin(float a, float b, float c, const float gamma)
{
    float ng = -gamma;
    a /= ng;
    b /= ng;
    c /= ng;
    float max_of = max(max(a, b), c);
    float sum = exp(a - max_of) + exp(b - max_of) + exp(c - max_of);

    return ng * (log(sum) + max_of);
}
\end{verbatim}

This function contains 17 floating point operations:

\begin{itemize}
  \item 1 x negation
  \item 3 x division
  \item 2 x max
  \item 3 x exponentiation
  \item 3 x subtraction
  \item 3 x addition
  \item 1 x natural logarithm
  \item 1 x multiplication
\end{itemize}

Additionally, the softmin of the previous 3 cell costs is added to the current
cell cost, for a total of 18 floating point operations (FLOPs). Therefore
Soft-DTW for a pair of time series of lengths $m$ and $n$ results in $18mn$
FLOPs.

\subsection{Test Hardware Specifications}
\FloatBarrier
\begin{table}[htbp]
  \centering
  \caption{Device Hardware Specifications}
  \label{specs}
  \begin{tabular}{lr}
      \toprule
          Property                & Value                           \\
          \midrule
          Model                   & GeForce RTX 2060 Super          \\
          Generation              & Turing                          \\
          Compute Capability      & 7.5                             \\
          SM Count                & 34                              \\
          CUDA Cores              & 2176                            \\
          Tensor Cores            & 272                             \\
          CUDA Version            & 11.2                            \\
          Driver Version          & 460.39                          \\
          Clock Speed (MHz)       & 1470                            \\
          Boost Speed (MHz)       & 1710                            \\
          Memory Speed (Gbps)     & 14                              \\
          Memory Size (GiB)       & 8                               \\
          Memory Bandwidth (GB/s) & 448                             \\
          L1 Cache (KiB) (per SM) & 64                              \\
          L2 Cache (KiB)          & 4096                            \\
          \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{Host Hardware Specifications}
  \label{hostspecs}
  \begin{tabular}{lrr}
      \toprule
          {} & Desktop \\
          \midrule
          RAM Size:            & 64GB                               \\
          Configuration:       & 16GB x 4                           \\
          Type:                & DDR4                               \\
          Speed:               & 2667 MT/s                          \\
          Bandwidth:           & 16415 MiB/s                        \\
          Architecture:        & x86\_64                            \\
          CPU(s):              & 16                                 \\
          Thread(s) per core:  & 2                                  \\
          Core(s) per socket:  & 8                                  \\
          Model name:          & AMD Ryzen 7 3700X 8-Core Processor \\
          CPU MHz:             & 1862.571                           \\
          CPU max MHz:         & 3600.0000                          \\
          CPU min MHz:         & 2200.0000                          \\
          BogoMIPS:            & 7187.14                            \\
          L1d cache:           & 256 KiB                            \\
          L1i cache:           & 256 KiB                            \\
          L2 cache:            & 4 MiB                              \\
          L3 cache:            & 32 MiB                             \\
          \bottomrule
  \end{tabular}
\end{table}

We tested the program on a single GeForce RTX 2060 Super GPU with the
specifications detailed in table \ref{specs} \cite{techpowerup}. Per NVIDIA's
documentation, ``The peak single precision floating point performance of a CUDA
device is defined as the number of CUDA cores times the graphics clock frequency
multiplied by two \cite{nvidia_flops}. For this device, the equation works out
to $2176 \times 1.71 \times 2 = 7441.92$ GFLOPS.

The GPU device is installed on a desktop computer running Ubuntu Linux 20.04
with CPU and memory specifications detailed in Table \ref{hostspecs}.


\begin{table}[htbp]
  \centering
  \caption{UWB LAB Device Hardware Specifications}
  \label{specslab}
  \begin{tabular}{lr}
      \toprule
          Property                & Value                           \\
          \midrule
          Model                   & GeForce GTX 745                 \\
          Compute Capability      & 3.0                             \\
          SM Count                & 3                               \\
          CUDA Cores              & 384                            \\
          CUDA Version            & 10.1                            \\
          Driver Version          & 10.1                          \\
          Clock Speed (MHz)       & 1032                            \\
          Memory clock rate(MHz)  & 14                              \\
          Memory Bandwidth (GB/s) & 2047                             \\
          L1 Cache (KiB) (per SM) & 64                              \\
          L2 Cache (KiB)          & 2048                             \\
          Maximum number of resident blocks per multiprocessor &16   \\ 
          Maximum amount of shared memory per multiprocessor(KB) & 48 \\
          Maximum number of resident blocks per multiprocessor &16 \\
          Maximum number of threads per multiprocessor  &2048 \\
          Total amount of shared memory per block (B)   &49152 \\
          \bottomrule
  \end{tabular}
\end{table}

for our experiments we also used a laboratory machine. The device has the compute capability 
3.0 with only 3 SM running on Centos Linux operating system.Table \ref{specslab} 
is showing the detail of UWB device. Due to the lower performance of the machine 
we preferred to use it for implementation of tiled and evaluating the tiled plus 
shared memory results. Therefore within this document whenever we talk about the 
performance of tiled kernel, we refer to this table for further analysis.  
\subsection{Test Data}

For performance testing we selected the ECG 200 dataset from the UCR Time Series
Archive \cite{dau_ucr_2019}. This dataset consists of 200 sets of
electrocardiogram time series of length 96. We also wrote a performance testing
program that generates sets of random unit normal data to test the performance
impact of varying time series length and count to many different sizes.


\section{Results}
\FloatBarrier

As a baseline for performance comparison, we timed the naive, sequential CPU
implementation of SoftDTW and found that its execution rate varied between
0.35-0.4 GFLOPs (Figure \ref{plot_cpu}) on the hardware described in Table
\ref{hostspecs}. While a multithreaded parallel CPU implementation could likely
achieve several times this rate of execution, we opted not to experiment with
CPU parallelism and instead move on to parallelizing the algorithm in CUDA with
GPU multithreading.

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.85}{\input{fig/plot_cpu.pgf}}
    \end{center}
    \caption{CPU performance on random unit normal time series length = 100}
    \label{plot_cpu}
\end{figure}

Figure \ref{plot_cpu_gpu} shows the execution rate of the naive CUDA kernel
vs. the CPU implementation for comparison. The CUDA kernel achieves performance
of around 34 GFLOPs compared to the CPU's 0.36 GFLOPs, a 92x performance
increase.

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.85}{\input{fig/plot_cpu_gpu.pgf}}
    \end{center}
    \caption{CPU vs. CUDA (Naive) performance on random unit normal time series
      length = 100}
    \label{plot_cpu_gpu}
\end{figure}


\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.85}{\input{fig/plot_multi.pgf}}
    \end{center}
    \caption{Kernel performance on random unit normal time series length =
      100}
    \label{plot_multi}
\end{figure}

After testing the naive kernel we experimented with CUDA performance
optimizations. Both the diagonal data layout and the shared memory stencil
brought significant performance improvements over the naive kernel, as seen in
Figure \ref{plot_multi}. While the naive kernel achieved GFLOPs in the low 30s,
the diagonal data layout transformation increased performance into the lower
40s, and the shared memory implementation reached almost 70 GFLOPs, essentially
double the execution rate of the naive kernel, for time series length 100.

Our performance measurements for the diagonal kernel stop at a problem size of
25000 pairwise DTW comparisons because the diagonal-major layout consumes more
space than the row-major layout (($m+n-1) \times min(m,n)$ matrix vs. an $m \times n$
matrix), exceeding the 8 GiB device memory capacity of the test GPU hardware.

Sakoe-Chiba bands also improved performance \emph{even after} adjusting for the
reduced number of total floating point operations; a bandwidth of 40 resulted in
the highest execution rate, exceeding 60 GFLOPs, compared to low-30s when the
bandwidth is set to 100 (Figure \ref{plot_bw}).

\begin{figure}[htbp]
    \begin{center}
        \scalebox{0.85}{\input{fig/plot_naive_multi_bw.pgf}}
    \end{center}
    \caption{Naive kernel performance on random unit normal data by
      Sakoe-Chiba bandwidth at time series length = 100}
    \label{plot_bw}
\end{figure}

The tiled kernel is using global barriers to handle the dependencies.  There is
one loop inside the kernel to manage the wave and another iteration within each
tile. In order to test the multivariate time series, we have to add two more
loops to create multi-stream and handle a bunch of time sequences. Adding this
option generates an overhead on the kernel and decreases the performance
significantly and we couldn't compare the result of the multivariate tiled
approach with other designed kernels. The main idea behind implementing a tiled
version is to compute the similarity of time series for longer sequences, while
other kernels are unable to process them. Figure \ref{plot_tiled} shows that,
for shorter length sequences the performance is less, but by increasing the
length, it is improving and capable of exceeding 120 GFLOPS.

\begin{figure}[htbp]
\includegraphics[height=4in]{fig/plot_tiled.png}
\centering
\caption{Soft DTW performance for Tiled \& shared memory kernel}
\label{plot_tiled}
\end{figure}


\FloatBarrier
\section{Profiling}

TODO

\input{fig/prof_table.tex}

\section{Discussion}

TODO

In order to improve performance, we convert kernels to multi-functional 
to be able to compute the entire dataset of time series within the kernel by one call. 
Therefore, the need for a nested loop to compare time series one by one removed, and the 
bulk of time series replaced that. Implementing this approach was challenging and required much effort. 

One of the major issues was the tiled kernel. Global barriers that we discussed in the 
optimization section lead to nested loops for managing the dependencies during intra-tile 
and inter-tile. The naive idea was to take the advantage of Cuda stream and run the loops 
concurrently. Although we were succeeded to implement this method, however, it was not much 
efficient and the performance compared to the other kernels was disappointing. This could be 
improved in the future by changes inside the kernel to cover the multivariate features. 
The primary strength of the tiled technique is its ability to calculate the similarity of 
two large time series with a logical performance. 
It has covered the limitation of other kernels for more than 1024 lengths. 
\section{Future Work}

Our library provides several individual CUDA kernels and C++ wrappers for
computing pairwise Soft-DTW dissimilarity measures in parallel. The library
could be further improved by creating additional kernels that apply the
optimizations to the Soft-DTW gradient computation as well, and that combine the
best-performing optimizations together. Due to time constraints we were unable
to explore the prefix computation technique described in Xiao et al (2013)
\cite{xiao_parallelizing_2013} and incorporating an implementation of this
algorithm remains a significant opportunity. Another library improvement would
be a batching implementation that handles much larger problem sizes within
limited device memory by launching kernels across multiple streams and
transferring data for the next problem while the previous one is being computed,
to hide latency.

Other potentially valuable future work includes integrating this library with an
optimization library that can iteratively minimize Soft-DTW cost to find
barycenters among a set of time series, to assemble a nearest centroid
classification system. Another area of potential is writing Python bindings to
expose the Soft-DTW loss and gradient functions to deep learning frameworks such
as TensorFlow or PyTorch, to enable the use of Soft-DTW loss as a training
objective for recurrent neural networks. This will facilitate tasks such as
classifying, predicting and generating sounds, gestures, and sensor data with
machine learning and neural networks, under the dynamic time warping geometry.

\printbibliography[]
\end{document}
